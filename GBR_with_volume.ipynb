{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTING REGRESSOR\n",
    "No ha salido muy bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40455 entries, 0 to 40454\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   carat    40455 non-null  float64\n",
      " 1   cut      40455 non-null  object \n",
      " 2   color    40455 non-null  object \n",
      " 3   clarity  40455 non-null  object \n",
      " 4   depth    40455 non-null  float64\n",
      " 5   table    40455 non-null  float64\n",
      " 6   price    40455 non-null  int64  \n",
      " 7   x        40455 non-null  float64\n",
      " 8   y        40455 non-null  float64\n",
      " 9   z        40455 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds_train = pd.read_csv('data/diamonds_train.csv')\n",
    "diamonds_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_predict = pd.read_csv('data/diamonds_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26155.000000</td>\n",
       "      <td>26155.000000</td>\n",
       "      <td>26155.000000</td>\n",
       "      <td>26155.000000</td>\n",
       "      <td>26155.000000</td>\n",
       "      <td>26155.000000</td>\n",
       "      <td>26155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.508951</td>\n",
       "      <td>61.737041</td>\n",
       "      <td>57.193871</td>\n",
       "      <td>1625.447639</td>\n",
       "      <td>5.042552</td>\n",
       "      <td>5.050535</td>\n",
       "      <td>3.115290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.198101</td>\n",
       "      <td>1.327113</td>\n",
       "      <td>2.212901</td>\n",
       "      <td>1114.414551</td>\n",
       "      <td>0.642784</td>\n",
       "      <td>0.640815</td>\n",
       "      <td>0.400528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>52.300000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.330000</td>\n",
       "      <td>61.100000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>2.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.460000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1179.000000</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>3.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>2304.000000</td>\n",
       "      <td>5.660000</td>\n",
       "      <td>5.660000</td>\n",
       "      <td>3.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>9636.000000</td>\n",
       "      <td>6.820000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>5.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  26155.000000  26155.000000  26155.000000  26155.000000  26155.000000   \n",
       "mean       0.508951     61.737041     57.193871   1625.447639      5.042552   \n",
       "std        0.198101      1.327113      2.212901   1114.414551      0.642784   \n",
       "min        0.200000     52.300000     44.000000    326.000000      0.000000   \n",
       "25%        0.330000     61.100000     56.000000    757.000000      4.440000   \n",
       "50%        0.460000     61.800000     57.000000   1179.000000      4.950000   \n",
       "75%        0.700000     62.500000     58.000000   2304.000000      5.660000   \n",
       "max        0.990000     79.000000     79.000000   9636.000000      6.820000   \n",
       "\n",
       "                  y             z  \n",
       "count  26155.000000  26155.000000  \n",
       "mean       5.050535      3.115290  \n",
       "std        0.640815      0.400528  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.450000      2.740000  \n",
       "50%        4.950000      3.050000  \n",
       "75%        5.660000      3.490000  \n",
       "max        6.740000      5.060000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train[diamonds_train['carat']<1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11512.000000</td>\n",
       "      <td>11512.000000</td>\n",
       "      <td>11512.000000</td>\n",
       "      <td>11512.000000</td>\n",
       "      <td>11512.000000</td>\n",
       "      <td>11512.000000</td>\n",
       "      <td>11512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.242681</td>\n",
       "      <td>61.754656</td>\n",
       "      <td>57.779960</td>\n",
       "      <td>7478.653579</td>\n",
       "      <td>6.867794</td>\n",
       "      <td>6.861926</td>\n",
       "      <td>4.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.226771</td>\n",
       "      <td>1.477758</td>\n",
       "      <td>2.086285</td>\n",
       "      <td>3178.403881</td>\n",
       "      <td>0.425233</td>\n",
       "      <td>0.419165</td>\n",
       "      <td>0.278123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.010000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>5018.750000</td>\n",
       "      <td>6.520000</td>\n",
       "      <td>6.520000</td>\n",
       "      <td>4.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.190000</td>\n",
       "      <td>61.900000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>6577.000000</td>\n",
       "      <td>6.770000</td>\n",
       "      <td>6.760000</td>\n",
       "      <td>4.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>62.600000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>9281.250000</td>\n",
       "      <td>7.240000</td>\n",
       "      <td>7.230000</td>\n",
       "      <td>4.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.990000</td>\n",
       "      <td>71.800000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>18806.000000</td>\n",
       "      <td>8.280000</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>5.120000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  11512.000000  11512.000000  11512.000000  11512.000000  11512.000000   \n",
       "mean       1.242681     61.754656     57.779960   7478.653579      6.867794   \n",
       "std        0.226771      1.477758      2.086285   3178.403881      0.425233   \n",
       "min        1.010000     43.000000     43.000000   2017.000000      0.000000   \n",
       "25%        1.040000     61.000000     56.000000   5018.750000      6.520000   \n",
       "50%        1.190000     61.900000     58.000000   6577.000000      6.770000   \n",
       "75%        1.500000     62.600000     59.000000   9281.250000      7.240000   \n",
       "max        1.990000     71.800000     70.000000  18806.000000      8.280000   \n",
       "\n",
       "                  y             z  \n",
       "count  11512.000000  11512.000000  \n",
       "mean       6.861926      4.236100  \n",
       "std        0.419165      0.278123  \n",
       "min        0.000000      0.000000  \n",
       "25%        6.520000      4.030000  \n",
       "50%        6.760000      4.180000  \n",
       "75%        7.230000      4.470000  \n",
       "max        8.170000      5.120000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train[(diamonds_train['carat']>1) & (diamonds_train['carat']<2)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "      <td>1643.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.135618</td>\n",
       "      <td>61.824346</td>\n",
       "      <td>58.324102</td>\n",
       "      <td>14816.021302</td>\n",
       "      <td>8.233177</td>\n",
       "      <td>8.240043</td>\n",
       "      <td>5.068503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.223455</td>\n",
       "      <td>1.885183</td>\n",
       "      <td>2.417542</td>\n",
       "      <td>2782.800988</td>\n",
       "      <td>0.354667</td>\n",
       "      <td>1.298732</td>\n",
       "      <td>0.366946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.300000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>5051.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.010000</td>\n",
       "      <td>60.700000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>13064.000000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>4.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.040000</td>\n",
       "      <td>61.900000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>15245.000000</td>\n",
       "      <td>8.180000</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>5.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.180000</td>\n",
       "      <td>62.700000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>17024.000000</td>\n",
       "      <td>8.360000</td>\n",
       "      <td>8.340000</td>\n",
       "      <td>5.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>70.600000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>8.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             carat        depth        table         price            x  \\\n",
       "count  1643.000000  1643.000000  1643.000000   1643.000000  1643.000000   \n",
       "mean      2.135618    61.824346    58.324102  14816.021302     8.233177   \n",
       "std       0.223455     1.885183     2.417542   2782.800988     0.354667   \n",
       "min       2.000000    55.300000    50.000000   5051.000000     0.000000   \n",
       "25%       2.010000    60.700000    57.000000  13064.000000     8.060000   \n",
       "50%       2.040000    61.900000    58.000000  15245.000000     8.180000   \n",
       "75%       2.180000    62.700000    60.000000  17024.000000     8.360000   \n",
       "max       4.500000    70.600000    95.000000  18823.000000    10.230000   \n",
       "\n",
       "                 y            z  \n",
       "count  1643.000000  1643.000000  \n",
       "mean      8.240043     5.068503  \n",
       "std       1.298732     0.366946  \n",
       "min       0.000000     0.000000  \n",
       "25%       8.050000     4.980000  \n",
       "50%       8.170000     5.050000  \n",
       "75%       8.340000     5.150000  \n",
       "max      58.900000     8.060000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train[diamonds_train['carat']>=2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_vars = diamonds_train.select_dtypes(include=[np.number]).columns\n",
    "#diamonds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING RID OF 0 IN xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40439, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train = diamonds_train[(diamonds_train['x'] != 0) & (diamonds_train['y'] != 0) & (diamonds_train['z'] != 0)]\n",
    "diamonds_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celda no ejecutada porque al eliminar rows en el csv de test, da fallo la submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diamonds_predict = diamonds_predict[(diamonds_predict['x'] != 0) & (diamonds_predict['y'] != 0) & (diamonds_predict['z'] != 0)]\n",
    "diamonds_predict.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET RID OF OUTLIERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried with 75-25, not a good idea shape=(6465, 10), we gonna try with some less outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay un diamante con un depth de 79\n",
    "diamonds_train = diamonds_train[(diamonds_train[\"depth\"]<71) & (diamonds_train[\"depth\"]>54)]\n",
    "\n",
    "# Hay 3 mayores de 75 y dos menores de 45\n",
    "diamonds_train = diamonds_train[(diamonds_train[\"table\"]<70) & (diamonds_train[\"table\"]>52)]\n",
    "\n",
    "diamonds_train = diamonds_train[(diamonds_train[\"x\"]<10)]  \n",
    "#solo dropeamos las que sean x<10 porque coincide con 'y' en sus dos únicos valores por encima de 10 y no dropeamos nada de z porque hay bastantes valores en torno a 6\n",
    "\n",
    "diamonds_train = diamonds_train[diamonds_train['carat'] < 2.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diamonds_train[diamonds_train['carat'] > 2.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40312, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOLUME COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['volume'] = diamonds_train.x * diamonds_train.y * diamonds_train.z\n",
    "diamonds_predict['volume'] = diamonds_predict.x * diamonds_predict.y * diamonds_predict.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13485 entries, 0 to 13484\n",
      "Data columns (total 11 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   id       13485 non-null  int64  \n",
      " 1   carat    13485 non-null  float64\n",
      " 2   cut      13485 non-null  object \n",
      " 3   color    13485 non-null  object \n",
      " 4   clarity  13485 non-null  object \n",
      " 5   depth    13485 non-null  float64\n",
      " 6   table    13485 non-null  float64\n",
      " 7   x        13485 non-null  float64\n",
      " 8   y        13485 non-null  float64\n",
      " 9   z        13485 non-null  float64\n",
      " 10  volume   13485 non-null  float64\n",
      "dtypes: float64(7), int64(1), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l/w ratio (x/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['l/w ratio'] = diamonds_train.x/diamonds_train.y\n",
    "diamonds_predict['l/w ratio'] = diamonds_predict.x/diamonds_predict.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40312.000000\n",
       "mean         0.999369\n",
       "std          0.011200\n",
       "min          0.137351\n",
       "25%          0.992593\n",
       "50%          0.995717\n",
       "75%          1.006920\n",
       "max          1.615572\n",
       "Name: l/w ratio, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds_train['l/w ratio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['depth ratio'] = diamonds_train.z/diamonds_train.y\n",
    "diamonds_predict['depth ratio'] = diamonds_predict.z/diamonds_predict.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log carat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['carat log'] = np.log(diamonds_train['carat'])\n",
    "diamonds_predict['carat log'] = np.log(diamonds_predict['carat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diamonds_train['density'] = diamonds_train['carat']/diamonds_train['volume']\n",
    "#diamonds_predict['density'] = diamonds_predict['carat']/diamonds_predict['volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diamonds_train = diamonds_train.replace([np.inf, -np.inf, 0], np.nan)\n",
    "#diamonds_predict = diamonds_predict.replace([np.inf, -np.inf, 0], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carat          0\n",
       "cut            0\n",
       "color          0\n",
       "clarity        0\n",
       "depth          0\n",
       "table          0\n",
       "price          0\n",
       "x              0\n",
       "y              0\n",
       "z              0\n",
       "volume         0\n",
       "l/w ratio      0\n",
       "depth ratio    0\n",
       "carat log      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diamonds_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diamonds_train.dropna(inplace=True)\n",
    "#diamonds_predict.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODING CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_num = {'Ideal': 5, 'Premium': 4, 'Very Good': 3, 'Good': 2, 'Fair': 1}\n",
    "diamonds_train['cut_num'] = diamonds_train['cut'].replace(cut_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_num = {'D': 7, 'E': 6, 'F': 5, 'G': 4, 'H': 3, 'I': 2, 'J': 1}\n",
    "diamonds_train['color_num'] = diamonds_train['color'].replace(color_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarity_num = {'IF': 8, 'VVS1': 7, 'VVS2': 6, 'VS1': 5, 'VS2': 4, 'SI1': 3, 'SI2': 2, 'I1': 1}\n",
    "diamonds_train['clarity_num'] = diamonds_train['clarity'].replace(clarity_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CSV (nuestro diamonds_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diamonds_predict['cut_num'] = diamonds_predict['cut'].replace(cut_num)\n",
    "\n",
    "diamonds_predict['color_num'] = diamonds_predict['color'].replace(color_num)\n",
    "\n",
    "diamonds_predict['clarity_num'] = diamonds_predict['clarity'].replace(clarity_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATEGORIES/CARAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['cut/crt'] = diamonds_train['cut_num']/diamonds_train['carat']\n",
    "diamonds_predict['cut/crt'] = diamonds_predict['cut_num']/diamonds_predict['carat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['color/crt'] = diamonds_train['color_num']/diamonds_train['carat']\n",
    "diamonds_predict['color/crt'] = diamonds_predict['color_num']/diamonds_predict['carat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train['clarity/crt'] = diamonds_train['clarity_num']/diamonds_train['carat']\n",
    "diamonds_predict['clarity/crt'] = diamonds_predict['clarity_num']/diamonds_predict['carat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13485 entries, 0 to 13484\n",
      "Data columns (total 20 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           13485 non-null  int64  \n",
      " 1   carat        13485 non-null  float64\n",
      " 2   cut          13485 non-null  object \n",
      " 3   color        13485 non-null  object \n",
      " 4   clarity      13485 non-null  object \n",
      " 5   depth        13485 non-null  float64\n",
      " 6   table        13485 non-null  float64\n",
      " 7   x            13485 non-null  float64\n",
      " 8   y            13485 non-null  float64\n",
      " 9   z            13485 non-null  float64\n",
      " 10  volume       13485 non-null  float64\n",
      " 11  l/w ratio    13482 non-null  float64\n",
      " 12  depth ratio  13482 non-null  float64\n",
      " 13  carat log    13485 non-null  float64\n",
      " 14  cut_num      13485 non-null  int64  \n",
      " 15  color_num    13485 non-null  int64  \n",
      " 16  clarity_num  13485 non-null  int64  \n",
      " 17  cut/crt      13485 non-null  float64\n",
      " 18  color/crt    13485 non-null  float64\n",
      " 19  clarity/crt  13485 non-null  float64\n",
      "dtypes: float64(13), int64(4), object(3)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds_predict.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diamonds_train['price/carat'] = (diamonds_train['price']/diamonds_train['carat']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEATMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth & cut no están muy relacionados con el precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "corr = diamonds_train.corr()\n",
    "sns.heatmap(data=corr, annot=True, cbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train.groupby('cut')[['x', 'y', 'z', 'table']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESCALAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_features = ['cut', 'color', 'clarity']\n",
    "features = ['carat', 'l/w ratio', 'volume', 'cut_num', 'color_num', 'clarity_num']\n",
    "#predict_feat = ['id','carat', 'table', 'depth', 'x', 'y', 'z', 'cut_num', 'color_num', 'clarity_num']\n",
    "target = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(diamonds_train[features])\n",
    "y = diamonds_train[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CSV (PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = scaler.fit_transform(diamonds_predict[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13485 entries, 0 to 13484\n",
      "Data columns (total 20 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           13485 non-null  int64  \n",
      " 1   carat        13485 non-null  float64\n",
      " 2   cut          13485 non-null  object \n",
      " 3   color        13485 non-null  object \n",
      " 4   clarity      13485 non-null  object \n",
      " 5   depth        13485 non-null  float64\n",
      " 6   table        13485 non-null  float64\n",
      " 7   x            13485 non-null  float64\n",
      " 8   y            13485 non-null  float64\n",
      " 9   z            13485 non-null  float64\n",
      " 10  volume       13485 non-null  float64\n",
      " 11  l/w ratio    13482 non-null  float64\n",
      " 12  depth ratio  13482 non-null  float64\n",
      " 13  carat log    13485 non-null  float64\n",
      " 14  cut_num      13485 non-null  int64  \n",
      " 15  color_num    13485 non-null  int64  \n",
      " 16  clarity_num  13485 non-null  int64  \n",
      " 17  cut/crt      13485 non-null  float64\n",
      " 18  color/crt    13485 non-null  float64\n",
      " 19  clarity/crt  13485 non-null  float64\n",
      "dtypes: float64(13), int64(4), object(3)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds_predict.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN-TEST-SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = diamonds_train[features]\n",
    "#y = diamonds_train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_model = RandomForestRegressor(n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=300)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score : 0.9823\n",
      "[0.98074232 0.98151686 0.97980522 0.98160776 0.98085216]\n",
      "mae: 261.619864\n",
      "R2: 0.982268\n",
      "mse: 279160.762988\n",
      "rmse: 528.356663\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(estimator=rfr_model, X=X_train, y=y_train, cv=5,verbose = 1)\n",
    "print('Cross validation score : %.4f' % rfr_model.score(X_test, y_test))\n",
    "print(cv_score)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "print(\"mae: %f\" %(mae))\n",
    "r2 = rfr_model.score(X_test,y_test)\n",
    "print(\"R2: %f\" %(r2))\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"mse: %f\" %(mse))\n",
    "rmse = (mean_squared_error(y_test,y_pred))**0.5\n",
    "print(\"rmse: %f\" %(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume without xyz\n",
    "Cross validation score : 0.9819\n",
    "[0.98231093 0.98064966 0.98158205 0.97886279 0.98079611]\n",
    "mae: 267.725271\n",
    "R2: 0.981928\n",
    "mse: 289137.876141\n",
    "rmse: 537.715423\n",
    "\"\"\"\n",
    "Cross validation score : 0.981\n",
    "[0.98114748 0.97993092 0.97976217 0.9798617  0.98153202]\n",
    "mae: 283.627434\n",
    "R2: 0.980543\n",
    "mse: 314089.953265\n",
    "rmse: 560.437288\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crossval_score = cross_val_score(rfr_model, X_train, y_train, cv=10)\n",
    "#print(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_model = DecisionTreeRegressor(random_state = 0) #rstate=0 porque los resultados son mejores que sin poner nada o poniendo 1\n",
    "dtr_model.fit(X_train, y_train)\n",
    "y_pred = dtr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(estimator=dtr_model, X=X_train, y=y_train, cv=5,verbose = 1)\n",
    "print('Cross validation score : %.3f' % dtr_model.score(X_test, y_test))\n",
    "print(cv_score)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "print(\"mae: %f\" %(mae))\n",
    "r2 = dtr_model.score(X_test,y_test)\n",
    "print(\"R2: %f\" %(r2))\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"mse: %f\" %(mse))\n",
    "rmse = (mean_squared_error(y_test,y_pred))**0.5\n",
    "print(\"rmse: %f\" %(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "print(\"mae: %f\" %(mae))\n",
    "r2 = rfr_model.score(X_test,y_test)\n",
    "print(\"R2: %f\" %(r2))\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"mse: %f\" %(mse))\n",
    "rmse = (mean_squared_error(y_test,y_pred))**0.5\n",
    "print(\"rmse: %f\" %(rmse))\n",
    "\n",
    "RESULTADOS SIN CAMBIAR RANDOM STATE:, con random state = 0, mejora\n",
    "mae: 372.840853\n",
    "R2: 0.980116\n",
    "mse: 579384.121723\n",
    "rmse: 761.172859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=7, loss='ls',verbose = 10)\n",
    "#max_depth 1, mal, 2, mal, 4 ok (553 rmse), 8 ok (542)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1    12928716.9000           18.55s\n",
      "         2    10567420.4358           17.21s\n",
      "         3     8651989.1387           16.23s\n",
      "         4     7096486.6262           15.62s\n",
      "         5     5831053.7898           14.80s\n",
      "         6     4802826.1258           14.27s\n",
      "         7     3966616.7416           14.15s\n",
      "         8     3285800.3941           13.80s\n",
      "         9     2729944.3593           13.52s\n",
      "        10     2277509.9917           13.29s\n",
      "        11     1906858.2769           13.21s\n",
      "        12     1603804.8894           13.02s\n",
      "        13     1356979.6734           12.85s\n",
      "        14     1157179.9499           12.69s\n",
      "        15      993314.1314           12.67s\n",
      "        16      858175.2572           12.54s\n",
      "        17      747175.8383           12.42s\n",
      "        18      655975.5944           12.29s\n",
      "        19      581265.7798           12.34s\n",
      "        20      520263.4343           12.22s\n",
      "        21      469441.6519           12.10s\n",
      "        22      427755.9153           11.99s\n",
      "        23      393104.3526           11.94s\n",
      "        24      364096.6255           11.84s\n",
      "        25      338978.4024           11.74s\n",
      "        26      317522.5506           11.64s\n",
      "        27      300299.0864           11.62s\n",
      "        28      285342.2429           11.52s\n",
      "        29      273379.6636           11.43s\n",
      "        30      262693.9830           11.34s\n",
      "        31      254248.1152           11.30s\n",
      "        32      247203.8227           11.21s\n",
      "        33      240266.0155           11.12s\n",
      "        34      234820.5878           11.02s\n",
      "        35      230256.1603           10.98s\n",
      "        36      226225.2392           10.90s\n",
      "        37      221962.0007           10.81s\n",
      "        38      218776.1749           10.73s\n",
      "        39      215900.9765           10.73s\n",
      "        40      212850.8721           10.64s\n",
      "        41      210359.7734           10.56s\n",
      "        42      208260.1147           10.48s\n",
      "        43      205285.5713           10.43s\n",
      "        44      203609.1154           10.35s\n",
      "        45      201347.3792           10.27s\n",
      "        46      199779.4576           10.20s\n",
      "        47      198173.1149           10.19s\n",
      "        48      195983.3638           10.11s\n",
      "        49      194352.2012           10.03s\n",
      "        50      192615.0166            9.95s\n",
      "        51      191238.5485            9.91s\n",
      "        52      189978.0191            9.83s\n",
      "        53      188785.5046            9.76s\n",
      "        54      187270.8290            9.68s\n",
      "        55      185852.8266            9.64s\n",
      "        56      184871.1048            9.57s\n",
      "        57      183879.8177            9.50s\n",
      "        58      182637.6229            9.43s\n",
      "        59      181606.3533            9.40s\n",
      "        60      180684.0904            9.33s\n",
      "        61      179230.1652            9.26s\n",
      "        62      178215.6267            9.19s\n",
      "        63      176954.3007            9.16s\n",
      "        64      175756.7249            9.10s\n",
      "        65      175386.4792            9.03s\n",
      "        66      174729.9918            8.96s\n",
      "        67      173623.1485            8.91s\n",
      "        68      172663.6847            8.84s\n",
      "        69      172044.2215            8.77s\n",
      "        70      171002.7800            8.70s\n",
      "        71      170058.4087            8.66s\n",
      "        72      168996.6612            8.59s\n",
      "        73      168261.2958            8.53s\n",
      "        74      167828.8499            8.46s\n",
      "        75      167165.6268            8.41s\n",
      "        76      166497.1299            8.35s\n",
      "        77      165737.4202            8.30s\n",
      "        78      164635.2368            8.25s\n",
      "        79      163372.5002            8.18s\n",
      "        80      162465.1694            8.11s\n",
      "        81      161489.6132            8.05s\n",
      "        82      160850.3319            7.99s\n",
      "        83      160080.7773            7.96s\n",
      "        84      159299.5036            7.90s\n",
      "        85      158755.7000            7.83s\n",
      "        86      157265.9656            7.76s\n",
      "        87      156208.5339            7.71s\n",
      "        88      155381.5312            7.64s\n",
      "        89      154687.7567            7.58s\n",
      "        90      154286.5719            7.53s\n",
      "        91      153323.0887            7.46s\n",
      "        92      152273.4681            7.39s\n",
      "        93      151138.0384            7.34s\n",
      "        94      149875.5017            7.27s\n",
      "        95      149384.6217            7.20s\n",
      "        96      149191.0357            7.14s\n",
      "        97      148725.6028            7.06s\n",
      "        98      147680.2221            6.99s\n",
      "        99      146761.0298            6.93s\n",
      "       100      146376.2000            6.86s\n",
      "       101      146063.8901            6.79s\n",
      "       102      145608.7525            6.74s\n",
      "       103      145358.6534            6.67s\n",
      "       104      144364.5882            6.60s\n",
      "       105      143508.6501            6.54s\n",
      "       106      142242.5614            6.47s\n",
      "       107      141582.8556            6.40s\n",
      "       108      141446.9323            6.35s\n",
      "       109      141215.9480            6.27s\n",
      "       110      140822.7599            6.20s\n",
      "       111      140007.8037            6.14s\n",
      "       112      139580.7046            6.06s\n",
      "       113      138981.5300            5.99s\n",
      "       114      138307.5430            5.94s\n",
      "       115      137944.9208            5.87s\n",
      "       116      137568.8522            5.80s\n",
      "       117      137274.3491            5.74s\n",
      "       118      137016.4264            5.70s\n",
      "       119      136209.6277            5.64s\n",
      "       120      135883.9939            5.57s\n",
      "       121      135525.5484            5.50s\n",
      "       122      134679.8847            5.43s\n",
      "       123      134232.0158            5.37s\n",
      "       124      134012.6108            5.30s\n",
      "       125      133535.4676            5.22s\n",
      "       126      132754.3389            5.15s\n",
      "       127      132569.3837            5.09s\n",
      "       128      132145.6422            5.02s\n",
      "       129      132050.5960            4.95s\n",
      "       130      131717.0875            4.87s\n",
      "       131      131348.8182            4.81s\n",
      "       132      130628.8574            4.74s\n",
      "       133      130306.2640            4.67s\n",
      "       134      129756.0429            4.60s\n",
      "       135      129466.4501            4.54s\n",
      "       136      129081.8560            4.47s\n",
      "       137      128924.5473            4.39s\n",
      "       138      128539.4209            4.32s\n",
      "       139      128360.5219            4.26s\n",
      "       140      127339.6219            4.19s\n",
      "       141      126907.4976            4.11s\n",
      "       142      126748.4675            4.04s\n",
      "       143      126161.3955            3.97s\n",
      "       144      124913.3239            3.90s\n",
      "       145      124343.4247            3.83s\n",
      "       146      124276.0416            3.77s\n",
      "       147      123999.8970            3.70s\n",
      "       148      123555.2856            3.64s\n",
      "       149      122696.2703            3.57s\n",
      "       150      121987.1327            3.50s\n",
      "       151      121672.5625            3.43s\n",
      "       152      121605.9037            3.36s\n",
      "       153      121382.5808            3.29s\n",
      "       154      120971.3916            3.22s\n",
      "       155      120664.1632            3.15s\n",
      "       156      119705.7568            3.08s\n",
      "       157      119302.7015            3.01s\n",
      "       158      118750.7838            2.94s\n",
      "       159      118504.3081            2.88s\n",
      "       160      118014.3879            2.81s\n",
      "       161      117673.8878            2.73s\n",
      "       162      117461.8941            2.67s\n",
      "       163      117137.2549            2.59s\n",
      "       164      116657.4120            2.52s\n",
      "       165      116600.7395            2.46s\n",
      "       166      116299.9033            2.39s\n",
      "       167      115727.8153            2.32s\n",
      "       168      115122.3190            2.25s\n",
      "       169      114933.4257            2.18s\n",
      "       170      114363.5350            2.10s\n",
      "       171      114260.2061            2.04s\n",
      "       172      114215.1422            1.97s\n",
      "       173      113825.0892            1.90s\n",
      "       174      113549.3344            1.83s\n",
      "       175      113513.2982            1.76s\n",
      "       176      113252.6550            1.69s\n",
      "       177      112721.8234            1.62s\n",
      "       178      111942.4171            1.55s\n",
      "       179      111261.7613            1.47s\n",
      "       180      110953.3727            1.41s\n",
      "       181      110693.2570            1.33s\n",
      "       182      110301.1760            1.26s\n",
      "       183      110159.7327            1.19s\n",
      "       184      109902.7097            1.12s\n",
      "       185      109539.8314            1.05s\n",
      "       186      109324.0164            0.98s\n",
      "       187      109160.2492            0.91s\n",
      "       188      108990.2524            0.84s\n",
      "       189      108736.0164            0.77s\n",
      "       190      108547.5932            0.70s\n",
      "       191      107952.2530            0.63s\n",
      "       192      107417.5875            0.56s\n",
      "       193      107347.1488            0.49s\n",
      "       194      107239.0472            0.42s\n",
      "       195      106740.0743            0.35s\n",
      "       196      106540.3607            0.28s\n",
      "       197      105846.9232            0.21s\n",
      "       198      105685.1102            0.14s\n",
      "       199      105342.5721            0.07s\n",
      "       200      105026.2532            0.00s\n"
     ]
    }
   ],
   "source": [
    "gbr_model.fit(X_train , y_train)\n",
    "y_pred = gbr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1    13000928.4729           11.88s\n",
      "         2    10627300.7402           11.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3     8698787.6998           10.95s\n",
      "         4     7133544.0533           11.03s\n",
      "         5     5859702.1017           11.22s\n",
      "         6     4822141.5726           10.90s\n",
      "         7     3981576.3777           10.85s\n",
      "         8     3293402.8912           11.08s\n",
      "         9     2734591.2212           10.99s\n",
      "        10     2280510.0796           10.78s\n",
      "        11     1910404.4436           10.61s\n",
      "        12     1606747.7090           10.61s\n",
      "        13     1358598.1854           10.66s\n",
      "        14     1157415.2721           10.52s\n",
      "        15      992225.7363           10.39s\n",
      "        16      856347.8165           10.35s\n",
      "        17      744895.9553           10.28s\n",
      "        18      654219.8272           10.17s\n",
      "        19      579466.4723           10.13s\n",
      "        20      517151.2790           10.12s\n",
      "        21      465224.6027           10.08s\n",
      "        22      422821.8674            9.97s\n",
      "        23      387646.6399            9.88s\n",
      "        24      358095.2188            9.83s\n",
      "        25      333718.6644            9.77s\n",
      "        26      311727.2201            9.67s\n",
      "        27      294109.0627            9.59s\n",
      "        28      279636.2951            9.54s\n",
      "        29      267122.2622            9.48s\n",
      "        30      255840.4476            9.40s\n",
      "        31      247252.7929            9.33s\n",
      "        32      240225.9598            9.34s\n",
      "        33      232517.2841            9.31s\n",
      "        34      225928.1934            9.23s\n",
      "        35      220382.4912            9.16s\n",
      "        36      215361.9549            9.11s\n",
      "        37      210891.2040            9.06s\n",
      "        38      207434.1807            8.98s\n",
      "        39      204765.5336            8.90s\n",
      "        40      201513.5376            8.85s\n",
      "        41      198460.3593            8.80s\n",
      "        42      196058.6530            8.72s\n",
      "        43      193705.8114            8.66s\n",
      "        44      191654.2927            8.65s\n",
      "        45      190009.9666            8.61s\n",
      "        46      188210.2131            8.54s\n",
      "        47      186562.5951            8.50s\n",
      "        48      185313.0627            8.45s\n",
      "        49      183881.8767            8.40s\n",
      "        50      182488.3138            8.33s\n",
      "        51      180419.5547            8.26s\n",
      "        52      179061.7377            8.23s\n",
      "        53      178113.7748            8.18s\n",
      "        54      176255.5174            8.11s\n",
      "        55      174913.6184            8.07s\n",
      "        56      173263.9020            8.06s\n",
      "        57      172253.1518            8.01s\n",
      "        58      170758.5218            7.94s\n",
      "        59      168624.4729            7.89s\n",
      "        60      167498.0276            7.84s\n",
      "        61      166379.5919            7.79s\n",
      "        62      165112.3960            7.72s\n",
      "        63      164475.3890            7.66s\n",
      "        64      163669.6780            7.64s\n",
      "        65      163289.3341            7.59s\n",
      "        66      162689.1011            7.52s\n",
      "        67      161146.4696            7.48s\n",
      "        68      159923.1380            7.44s\n",
      "        69      159450.9693            7.40s\n",
      "        70      157611.8087            7.33s\n",
      "        71      156585.8680            7.27s\n",
      "        72      155447.7477            7.22s\n",
      "        73      154661.1362            7.19s\n",
      "        74      153150.1167            7.13s\n",
      "        75      151847.5551            7.09s\n",
      "        76      150497.0544            7.06s\n",
      "        77      149500.9643            7.00s\n",
      "        78      147977.0137            6.93s\n",
      "        79      147073.1699            6.87s\n",
      "        80      145492.5435            6.82s\n",
      "        81      144902.5053            6.77s\n",
      "        82      144023.8155            6.71s\n",
      "        83      143757.0394            6.66s\n",
      "        84      143103.0377            6.61s\n",
      "        85      142202.3787            6.55s\n",
      "        86      141563.2687            6.49s\n",
      "        87      140270.3002            6.43s\n",
      "        88      139428.5104            6.38s\n",
      "        89      138684.7596            6.33s\n",
      "        90      138087.5408            6.27s\n",
      "        91      136505.6651            6.22s\n",
      "        92      136306.5049            6.19s\n",
      "        93      135562.7795            6.13s\n",
      "        94      134828.2256            6.06s\n",
      "        95      134443.1743            6.01s\n",
      "        96      133118.9664            5.95s\n",
      "        97      132893.8319            5.90s\n",
      "        98      132499.2369            5.84s\n",
      "        99      132026.8200            5.78s\n",
      "       100      130982.6224            5.73s\n",
      "       101      130269.2407            5.68s\n",
      "       102      129768.1986            5.62s\n",
      "       103      129267.5428            5.56s\n",
      "       104      128663.7010            5.51s\n",
      "       105      127855.7374            5.45s\n",
      "       106      127160.8151            5.39s\n",
      "       107      126875.8230            5.33s\n",
      "       108      126265.3544            5.27s\n",
      "       109      125775.2937            5.21s\n",
      "       110      125413.2258            5.15s\n",
      "       111      124743.4748            5.09s\n",
      "       112      124228.4338            5.04s\n",
      "       113      123528.6529            4.99s\n",
      "       114      122956.2302            4.93s\n",
      "       115      122494.5176            4.87s\n",
      "       116      122292.6458            4.82s\n",
      "       117      121703.1108            4.77s\n",
      "       118      121349.5366            4.71s\n",
      "       119      121038.2618            4.65s\n",
      "       120      120988.2990            4.59s\n",
      "       121      120695.4818            4.54s\n",
      "       122      120293.4090            4.48s\n",
      "       123      120084.6389            4.42s\n",
      "       124      119638.3441            4.36s\n",
      "       125      119028.9378            4.31s\n",
      "       126      118839.9736            4.25s\n",
      "       127      118223.9867            4.19s\n",
      "       128      117883.9722            4.13s\n",
      "       129      117502.0322            4.08s\n",
      "       130      117111.5027            4.02s\n",
      "       131      116941.4549            3.96s\n",
      "       132      116912.5236            3.90s\n",
      "       133      116308.0423            3.85s\n",
      "       134      116067.6106            3.79s\n",
      "       135      115883.0524            3.73s\n",
      "       136      115264.0783            3.67s\n",
      "       137      114542.3108            3.62s\n",
      "       138      113945.1416            3.56s\n",
      "       139      113552.7259            3.50s\n",
      "       140      113277.9025            3.45s\n",
      "       141      112690.5619            3.40s\n",
      "       142      112139.8736            3.34s\n",
      "       143      111979.0803            3.28s\n",
      "       144      111454.8554            3.23s\n",
      "       145      111433.5651            3.17s\n",
      "       146      111049.5998            3.11s\n",
      "       147      110213.6295            3.05s\n",
      "       148      109618.2432            2.99s\n",
      "       149      109356.1749            2.94s\n",
      "       150      108836.7605            2.88s\n",
      "       151      108415.6764            2.82s\n",
      "       152      108202.4268            2.77s\n",
      "       153      108134.7864            2.71s\n",
      "       154      107362.3800            2.65s\n",
      "       155      107102.6118            2.59s\n",
      "       156      106688.1890            2.54s\n",
      "       157      106585.4011            2.48s\n",
      "       158      106025.5594            2.42s\n",
      "       159      105894.8503            2.36s\n",
      "       160      105741.3511            2.31s\n",
      "       161      105297.3850            2.25s\n",
      "       162      104955.7121            2.19s\n",
      "       163      104836.7996            2.13s\n",
      "       164      104637.4568            2.08s\n",
      "       165      104091.2064            2.02s\n",
      "       166      103967.3100            1.96s\n",
      "       167      103644.4664            1.90s\n",
      "       168      103304.4855            1.85s\n",
      "       169      102923.7573            1.79s\n",
      "       170      102839.3129            1.73s\n",
      "       171      102360.1806            1.67s\n",
      "       172      101875.4110            1.62s\n",
      "       173      101189.2220            1.56s\n",
      "       174      101057.7654            1.50s\n",
      "       175      100605.0171            1.44s\n",
      "       176      100175.1898            1.39s\n",
      "       177       99826.4536            1.33s\n",
      "       178       99467.9781            1.28s\n",
      "       179       99363.3941            1.22s\n",
      "       180       99056.5272            1.16s\n",
      "       181       98624.9876            1.10s\n",
      "       182       98291.7203            1.05s\n",
      "       183       98190.6880            0.99s\n",
      "       184       98025.4608            0.93s\n",
      "       185       97806.2954            0.87s\n",
      "       186       97126.6010            0.81s\n",
      "       187       96900.8185            0.75s\n",
      "       188       96270.9939            0.70s\n",
      "       189       95926.6457            0.64s\n",
      "       190       95683.7310            0.58s\n",
      "       191       95419.2198            0.52s\n",
      "       192       94746.8807            0.47s\n",
      "       193       94584.2846            0.41s\n",
      "       194       94175.3507            0.35s\n",
      "       195       93776.6827            0.29s\n",
      "       196       93309.6647            0.23s\n",
      "       197       92850.4817            0.17s\n",
      "       198       92318.6437            0.12s\n",
      "       199       92263.8217            0.06s\n",
      "       200       91899.1763            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    12931980.0205           10.57s\n",
      "         2    10569607.6020           11.43s\n",
      "         3     8652654.3940           10.87s\n",
      "         4     7096147.5867           10.83s\n",
      "         5     5830195.1323           10.80s\n",
      "         6     4801728.2541           10.97s\n",
      "         7     3964738.0044           10.93s\n",
      "         8     3282612.5772           10.94s\n",
      "         9     2725453.0576           10.75s\n",
      "        10     2273180.7506           10.71s\n",
      "        11     1903708.4050           10.55s\n",
      "        12     1601870.7578           10.51s\n",
      "        13     1355034.5892           10.42s\n",
      "        14     1153052.2942           10.46s\n",
      "        15      988470.5062           10.34s\n",
      "        16      853600.2513           10.32s\n",
      "        17      742534.3045           10.27s\n",
      "        18      652382.3452           10.25s\n",
      "        19      576858.8494           10.14s\n",
      "        20      514780.2969           10.09s\n",
      "        21      464098.1771           10.00s\n",
      "        22      422034.1083            9.95s\n",
      "        23      386663.1828            9.89s\n",
      "        24      357557.0950            9.89s\n",
      "        25      332200.6379            9.84s\n",
      "        26      310977.6120            9.81s\n",
      "        27      294003.6728            9.72s\n",
      "        28      279554.6094            9.65s\n",
      "        29      266890.4839            9.59s\n",
      "        30      255791.9572            9.57s\n",
      "        31      246215.9751            9.52s\n",
      "        32      238820.9939            9.49s\n",
      "        33      232233.2328            9.41s\n",
      "        34      226263.6926            9.35s\n",
      "        35      220797.4162            9.29s\n",
      "        36      216592.0739            9.26s\n",
      "        37      212812.7969            9.22s\n",
      "        38      208355.6971            9.16s\n",
      "        39      205213.1070            9.12s\n",
      "        40      202023.3417            9.08s\n",
      "        41      198764.6198            9.04s\n",
      "        42      196633.4737            8.99s\n",
      "        43      193557.8612            8.97s\n",
      "        44      191582.1604            8.94s\n",
      "        45      189594.0561            8.87s\n",
      "        46      188031.9837            8.81s\n",
      "        47      185630.1425            8.73s\n",
      "        48      184168.2546            8.68s\n",
      "        49      182663.9271            8.60s\n",
      "        50      180611.3975            8.55s\n",
      "        51      179413.0746            8.50s\n",
      "        52      178174.4063            8.46s\n",
      "        53      177055.5000            8.39s\n",
      "        54      175918.4998            8.34s\n",
      "        55      174140.5482            8.27s\n",
      "        56      173259.3886            8.22s\n",
      "        57      172361.0234            8.15s\n",
      "        58      171341.3211            8.10s\n",
      "        59      169823.5393            8.06s\n",
      "        60      168561.0211            8.01s\n",
      "        61      167605.4534            7.98s\n",
      "        62      166735.5447            7.92s\n",
      "        63      164908.8741            7.89s\n",
      "        64      163491.4090            7.83s\n",
      "        65      162133.2716            7.77s\n",
      "        66      160965.4861            7.72s\n",
      "        67      159125.3260            7.65s\n",
      "        68      158728.8003            7.60s\n",
      "        69      158018.3093            7.55s\n",
      "        70      156703.1879            7.49s\n",
      "        71      155800.7572            7.43s\n",
      "        72      155058.1967            7.37s\n",
      "        73      154113.7119            7.33s\n",
      "        74      153244.8119            7.28s\n",
      "        75      151841.8595            7.22s\n",
      "        76      150995.5663            7.16s\n",
      "        77      149991.2074            7.10s\n",
      "        78      148868.4641            7.05s\n",
      "        79      148095.9114            6.98s\n",
      "        80      147590.9944            6.93s\n",
      "        81      146933.4955            6.86s\n",
      "        82      145482.8083            6.81s\n",
      "        83      144967.7713            6.74s\n",
      "        84      144589.7869            6.69s\n",
      "        85      143420.0875            6.63s\n",
      "        86      142415.1502            6.57s\n",
      "        87      141924.8707            6.51s\n",
      "        88      140978.1525            6.46s\n",
      "        89      139790.8320            6.41s\n",
      "        90      139458.5485            6.35s\n",
      "        91      138659.8027            6.31s\n",
      "        92      138111.2265            6.25s\n",
      "        93      137772.7407            6.20s\n",
      "        94      137100.5154            6.15s\n",
      "        95      135915.7099            6.09s\n",
      "        96      135357.4984            6.03s\n",
      "        97      134569.2239            5.97s\n",
      "        98      133996.3094            5.91s\n",
      "        99      132927.5728            5.85s\n",
      "       100      132217.8563            5.79s\n",
      "       101      131864.7309            5.73s\n",
      "       102      131072.6913            5.67s\n",
      "       103      130675.8810            5.60s\n",
      "       104      130332.0100            5.55s\n",
      "       105      129480.4750            5.49s\n",
      "       106      128985.9197            5.43s\n",
      "       107      128710.2490            5.37s\n",
      "       108      128009.6386            5.31s\n",
      "       109      127711.0535            5.25s\n",
      "       110      126816.8010            5.20s\n",
      "       111      126530.5145            5.14s\n",
      "       112      126036.5954            5.09s\n",
      "       113      125411.7647            5.03s\n",
      "       114      125191.9125            4.97s\n",
      "       115      123875.1618            4.91s\n",
      "       116      123185.0610            4.85s\n",
      "       117      123002.6707            4.80s\n",
      "       118      122534.0016            4.74s\n",
      "       119      121634.5660            4.68s\n",
      "       120      121410.1253            4.62s\n",
      "       121      121013.1334            4.57s\n",
      "       122      120615.8011            4.51s\n",
      "       123      120552.0400            4.46s\n",
      "       124      119283.8990            4.40s\n",
      "       125      118885.2471            4.34s\n",
      "       126      118210.7170            4.28s\n",
      "       127      117942.4585            4.23s\n",
      "       128      117692.4650            4.19s\n",
      "       129      117498.4254            4.14s\n",
      "       130      117219.7767            4.09s\n",
      "       131      116947.4327            4.03s\n",
      "       132      116557.8640            3.98s\n",
      "       133      115587.8311            3.92s\n",
      "       134      114821.9468            3.86s\n",
      "       135      114345.2724            3.80s\n",
      "       136      113763.4337            3.74s\n",
      "       137      113209.8774            3.68s\n",
      "       138      113036.8663            3.62s\n",
      "       139      112799.6309            3.56s\n",
      "       140      112632.3658            3.51s\n",
      "       141      112085.1966            3.45s\n",
      "       142      111889.3506            3.39s\n",
      "       143      111732.4942            3.34s\n",
      "       144      111366.3752            3.28s\n",
      "       145      110947.7363            3.22s\n",
      "       146      110364.8638            3.16s\n",
      "       147      109532.7470            3.10s\n",
      "       148      109372.5047            3.04s\n",
      "       149      109069.2310            2.98s\n",
      "       150      108825.0901            2.92s\n",
      "       151      108700.4365            2.87s\n",
      "       152      108491.2334            2.81s\n",
      "       153      107854.2197            2.75s\n",
      "       154      107115.5592            2.69s\n",
      "       155      106593.8292            2.63s\n",
      "       156      106423.6420            2.58s\n",
      "       157      106002.1001            2.52s\n",
      "       158      105666.5228            2.46s\n",
      "       159      104881.1652            2.40s\n",
      "       160      104752.9893            2.35s\n",
      "       161      104604.3613            2.29s\n",
      "       162      104568.6779            2.23s\n",
      "       163      104166.3706            2.17s\n",
      "       164      103737.0010            2.11s\n",
      "       165      103638.7859            2.05s\n",
      "       166      103489.5277            1.99s\n",
      "       167      103342.2181            1.94s\n",
      "       168      102336.4205            1.88s\n",
      "       169      102260.0416            1.82s\n",
      "       170      101882.8352            1.76s\n",
      "       171      101760.1981            1.70s\n",
      "       172      101293.5965            1.64s\n",
      "       173      100635.0564            1.58s\n",
      "       174      100243.2632            1.53s\n",
      "       175       99882.3745            1.47s\n",
      "       176       99752.1788            1.41s\n",
      "       177       99420.0675            1.35s\n",
      "       178       99311.9207            1.29s\n",
      "       179       98698.1954            1.23s\n",
      "       180       98609.9622            1.18s\n",
      "       181       98326.9920            1.12s\n",
      "       182       98081.7360            1.06s\n",
      "       183       97840.9596            1.00s\n",
      "       184       97751.2822            0.94s\n",
      "       185       97660.8582            0.88s\n",
      "       186       97452.6231            0.82s\n",
      "       187       97133.4041            0.77s\n",
      "       188       96854.5041            0.71s\n",
      "       189       96626.8973            0.65s\n",
      "       190       96248.4483            0.59s\n",
      "       191       95605.7097            0.53s\n",
      "       192       95436.5407            0.47s\n",
      "       193       95156.1337            0.41s\n",
      "       194       94866.9937            0.35s\n",
      "       195       94593.5612            0.29s\n",
      "       196       94428.0204            0.24s\n",
      "       197       94373.1792            0.18s\n",
      "       198       94205.1831            0.12s\n",
      "       199       94189.0266            0.06s\n",
      "       200       93738.6653            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    12940478.7544            9.95s\n",
      "         2    10574177.9363           10.74s\n",
      "         3     8654669.6702           10.81s\n",
      "         4     7098039.4422           10.52s\n",
      "         5     5830754.9521           10.29s\n",
      "         6     4802021.5468           10.33s\n",
      "         7     3964493.6353           10.36s\n",
      "         8     3283489.6928           10.19s\n",
      "         9     2726888.4017           10.14s\n",
      "        10     2273646.5554           10.28s\n",
      "        11     1903852.1906           10.37s\n",
      "        12     1601354.7308           10.35s\n",
      "        13     1354469.9990           10.23s\n",
      "        14     1153499.6882           10.19s\n",
      "        15      988183.6049           10.19s\n",
      "        16      852804.2220           10.07s\n",
      "        17      741584.8132            9.97s\n",
      "        18      650336.4243            9.93s\n",
      "        19      576584.6290            9.90s\n",
      "        20      515330.8659            9.80s\n",
      "        21      464185.2605            9.71s\n",
      "        22      420862.8761            9.67s\n",
      "        23      385713.8203            9.63s\n",
      "        24      356417.0589            9.54s\n",
      "        25      331739.2087            9.48s\n",
      "        26      311375.4394            9.54s\n",
      "        27      294556.6445            9.54s\n",
      "        28      279882.2150            9.46s\n",
      "        29      267608.9350            9.45s\n",
      "        30      256583.3827            9.49s\n",
      "        31      247526.8286            9.45s\n",
      "        32      240306.0514            9.36s\n",
      "        33      233627.7095            9.28s\n",
      "        34      228103.7707            9.24s\n",
      "        35      222010.1864            9.19s\n",
      "        36      217965.8544            9.11s\n",
      "        37      213716.8975            9.03s\n",
      "        38      210468.3442            9.00s\n",
      "        39      207129.4659            8.99s\n",
      "        40      203314.3144            8.91s\n",
      "        41      200387.5459            8.87s\n",
      "        42      196994.5373            8.89s\n",
      "        43      195305.0101            8.84s\n",
      "        44      192937.6081            8.77s\n",
      "        45      190721.8953            8.69s\n",
      "        46      189202.6383            8.64s\n",
      "        47      187587.3603            8.61s\n",
      "        48      185955.8906            8.53s\n",
      "        49      184877.7743            8.46s\n",
      "        50      183358.1550            8.42s\n",
      "        51      182133.8145            8.37s\n",
      "        52      179398.7465            8.30s\n",
      "        53      177301.3714            8.25s\n",
      "        54      175017.1228            8.23s\n",
      "        55      173257.0040            8.17s\n",
      "        56      172663.3679            8.11s\n",
      "        57      170800.2914            8.07s\n",
      "        58      169395.9192            8.04s\n",
      "        59      168505.0621            7.99s\n",
      "        60      167649.6864            7.93s\n",
      "        61      166582.5646            7.86s\n",
      "        62      165773.7031            7.84s\n",
      "        63      164697.7457            7.79s\n",
      "        64      163776.0458            7.72s\n",
      "        65      162902.9915            7.69s\n",
      "        66      161643.3642            7.65s\n",
      "        67      160711.7025            7.59s\n",
      "        68      158207.7226            7.53s\n",
      "        69      157527.7239            7.47s\n",
      "        70      156266.5030            7.42s\n",
      "        71      155600.1956            7.37s\n",
      "        72      154362.9403            7.31s\n",
      "        73      153832.5146            7.25s\n",
      "        74      153490.7833            7.22s\n",
      "        75      151586.3619            7.17s\n",
      "        76      150583.3049            7.10s\n",
      "        77      149282.5154            7.05s\n",
      "        78      148178.9347            7.01s\n",
      "        79      147118.2130            6.96s\n",
      "        80      146853.3282            6.90s\n",
      "        81      146258.4339            6.84s\n",
      "        82      144966.3005            6.80s\n",
      "        83      144394.9536            6.74s\n",
      "        84      142856.5993            6.68s\n",
      "        85      141998.6663            6.62s\n",
      "        86      140906.3661            6.57s\n",
      "        87      140209.7594            6.51s\n",
      "        88      139591.8846            6.45s\n",
      "        89      138925.2813            6.40s\n",
      "        90      138309.1581            6.36s\n",
      "        91      137574.8041            6.31s\n",
      "        92      136874.0483            6.25s\n",
      "        93      136302.0939            6.19s\n",
      "        94      135348.7197            6.14s\n",
      "        95      134434.2786            6.09s\n",
      "        96      134212.2836            6.03s\n",
      "        97      133812.2921            5.97s\n",
      "        98      132878.6002            5.92s\n",
      "        99      132214.7314            5.87s\n",
      "       100      131153.3300            5.81s\n",
      "       101      130829.4019            5.75s\n",
      "       102      130344.9400            5.70s\n",
      "       103      129725.2761            5.65s\n",
      "       104      129397.2740            5.59s\n",
      "       105      129020.0465            5.53s\n",
      "       106      128656.9412            5.47s\n",
      "       107      127965.3062            5.41s\n",
      "       108      127523.6014            5.35s\n",
      "       109      127164.7406            5.29s\n",
      "       110      126941.0966            5.25s\n",
      "       111      126703.0757            5.19s\n",
      "       112      126438.2217            5.13s\n",
      "       113      126077.5945            5.07s\n",
      "       114      125788.8565            5.01s\n",
      "       115      125431.7067            4.95s\n",
      "       116      124555.2170            4.89s\n",
      "       117      124247.2331            4.84s\n",
      "       118      123785.5411            4.78s\n",
      "       119      123250.1641            4.73s\n",
      "       120      122998.0488            4.66s\n",
      "       121      122807.1286            4.61s\n",
      "       122      122563.9443            4.55s\n",
      "       123      121845.6016            4.50s\n",
      "       124      121749.7892            4.44s\n",
      "       125      121540.4642            4.39s\n",
      "       126      121178.1330            4.34s\n",
      "       127      120844.1451            4.28s\n",
      "       128      120312.9433            4.22s\n",
      "       129      119551.9787            4.16s\n",
      "       130      119152.8505            4.11s\n",
      "       131      118697.6958            4.05s\n",
      "       132      117815.7952            3.99s\n",
      "       133      117553.3757            3.93s\n",
      "       134      116728.3845            3.87s\n",
      "       135      116533.9490            3.81s\n",
      "       136      115628.6071            3.75s\n",
      "       137      115297.5358            3.69s\n",
      "       138      114156.9146            3.63s\n",
      "       139      113944.5309            3.57s\n",
      "       140      113778.4706            3.51s\n",
      "       141      113557.9790            3.46s\n",
      "       142      112951.4431            3.41s\n",
      "       143      112852.4953            3.35s\n",
      "       144      112305.9722            3.29s\n",
      "       145      111922.8607            3.23s\n",
      "       146      111495.1301            3.17s\n",
      "       147      111255.0190            3.11s\n",
      "       148      110216.8475            3.05s\n",
      "       149      109534.9918            2.99s\n",
      "       150      108845.2781            2.94s\n",
      "       151      108493.5733            2.88s\n",
      "       152      108304.0941            2.82s\n",
      "       153      108167.9974            2.77s\n",
      "       154      107839.2310            2.71s\n",
      "       155      107653.2319            2.65s\n",
      "       156      107271.2589            2.59s\n",
      "       157      107130.3470            2.53s\n",
      "       158      106841.7064            2.47s\n",
      "       159      106640.8225            2.41s\n",
      "       160      106079.8697            2.35s\n",
      "       161      105839.3493            2.29s\n",
      "       162      105521.1502            2.24s\n",
      "       163      105154.5692            2.18s\n",
      "       164      104957.3252            2.12s\n",
      "       165      104404.2648            2.06s\n",
      "       166      104075.8136            2.01s\n",
      "       167      103932.8448            1.95s\n",
      "       168      103033.5702            1.89s\n",
      "       169      102716.6614            1.83s\n",
      "       170      102548.6942            1.77s\n",
      "       171      102172.5388            1.71s\n",
      "       172      101960.9846            1.65s\n",
      "       173      101385.0968            1.59s\n",
      "       174      101173.7579            1.54s\n",
      "       175      100868.2930            1.48s\n",
      "       176      100570.8008            1.42s\n",
      "       177      100467.8524            1.36s\n",
      "       178      100310.3477            1.30s\n",
      "       179      100029.1997            1.24s\n",
      "       180       99952.3313            1.18s\n",
      "       181       99630.1791            1.12s\n",
      "       182       99161.7140            1.06s\n",
      "       183       98805.7302            1.01s\n",
      "       184       98556.3622            0.95s\n",
      "       185       98009.4353            0.89s\n",
      "       186       97632.8209            0.83s\n",
      "       187       97327.7665            0.77s\n",
      "       188       96614.1240            0.71s\n",
      "       189       96258.1011            0.65s\n",
      "       190       96111.7870            0.59s\n",
      "       191       95716.7658            0.53s\n",
      "       192       95543.4579            0.47s\n",
      "       193       94696.0758            0.41s\n",
      "       194       94389.4327            0.36s\n",
      "       195       93833.9626            0.30s\n",
      "       196       93752.6237            0.24s\n",
      "       197       93625.8898            0.18s\n",
      "       198       93385.1412            0.12s\n",
      "       199       93318.3244            0.06s\n",
      "       200       92707.0475            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    12897854.2206           13.42s\n",
      "         2    10544039.7179           12.03s\n",
      "         3     8630912.3010           11.25s\n",
      "         4     7078947.0664           10.85s\n",
      "         5     5815436.2095           10.84s\n",
      "         6     4789540.1948           10.58s\n",
      "         7     3953075.0245           10.38s\n",
      "         8     3275568.1996           10.23s\n",
      "         9     2722358.3151           10.24s\n",
      "        10     2270732.8398           10.14s\n",
      "        11     1901431.4805           10.02s\n",
      "        12     1598935.2726            9.92s\n",
      "        13     1352641.6743            9.92s\n",
      "        14     1152699.7141            9.82s\n",
      "        15      988480.4119            9.73s\n",
      "        16      854227.9748            9.65s\n",
      "        17      743786.7049            9.62s\n",
      "        18      652478.9621            9.54s\n",
      "        19      578415.7991            9.46s\n",
      "        20      516442.0639            9.39s\n",
      "        21      465934.9138            9.47s\n",
      "        22      422709.3626            9.42s\n",
      "        23      387334.7210            9.34s\n",
      "        24      358123.8348            9.32s\n",
      "        25      333501.5019            9.36s\n",
      "        26      313315.0931            9.28s\n",
      "        27      296563.8374            9.20s\n",
      "        28      280960.2291            9.16s\n",
      "        29      268326.9839            9.19s\n",
      "        30      258359.9609            9.11s\n",
      "        31      248746.8480            9.04s\n",
      "        32      240770.6629            9.00s\n",
      "        33      233809.1579            9.01s\n",
      "        34      227593.3538            8.94s\n",
      "        35      222175.4993            8.86s\n",
      "        36      217749.4149            8.80s\n",
      "        37      213901.9083            8.75s\n",
      "        38      210631.9594            8.68s\n",
      "        39      207466.5008            8.61s\n",
      "        40      203611.8096            8.55s\n",
      "        41      201223.7936            8.50s\n",
      "        42      198465.3885            8.44s\n",
      "        43      196504.5846            8.37s\n",
      "        44      194216.1391            8.32s\n",
      "        45      191887.1402            8.32s\n",
      "        46      189580.0997            8.27s\n",
      "        47      187608.8373            8.20s\n",
      "        48      186283.6282            8.17s\n",
      "        49      184540.4726            8.15s\n",
      "        50      183328.6592            8.08s\n",
      "        51      181314.2544            8.02s\n",
      "        52      179527.0291            7.96s\n",
      "        53      177714.3429            7.93s\n",
      "        54      175839.6741            7.87s\n",
      "        55      173725.1822            7.81s\n",
      "        56      172370.7266            7.76s\n",
      "        57      170222.2834            7.75s\n",
      "        58      168536.7788            7.69s\n",
      "        59      167183.3248            7.63s\n",
      "        60      165276.0527            7.61s\n",
      "        61      164597.6014            7.58s\n",
      "        62      163514.2955            7.52s\n",
      "        63      162219.2073            7.46s\n",
      "        64      161357.8695            7.40s\n",
      "        65      160224.5849            7.36s\n",
      "        66      159242.1670            7.30s\n",
      "        67      158085.2770            7.24s\n",
      "        68      156765.1584            7.19s\n",
      "        69      156273.3504            7.14s\n",
      "        70      155122.9857            7.09s\n",
      "        71      154061.2644            7.03s\n",
      "        72      152955.3850            6.98s\n",
      "        73      152222.6242            6.96s\n",
      "        74      151519.5406            6.90s\n",
      "        75      150681.2828            6.84s\n",
      "        76      150049.1755            6.79s\n",
      "        77      149325.2539            6.74s\n",
      "        78      148285.2884            6.68s\n",
      "        79      147171.9343            6.63s\n",
      "        80      145808.2520            6.58s\n",
      "        81      144849.5998            6.55s\n",
      "        82      143934.9429            6.49s\n",
      "        83      143054.9100            6.43s\n",
      "        84      142496.0491            6.37s\n",
      "        85      142071.7624            6.32s\n",
      "        86      141703.6637            6.27s\n",
      "        87      141235.7781            6.21s\n",
      "        88      140511.7107            6.16s\n",
      "        89      139840.4233            6.12s\n",
      "        90      138945.2745            6.07s\n",
      "        91      138426.4440            6.01s\n",
      "        92      137918.1833            5.97s\n",
      "        93      137513.0433            5.93s\n",
      "        94      137011.4814            5.87s\n",
      "        95      136137.9651            5.81s\n",
      "        96      134813.8547            5.75s\n",
      "        97      134504.8525            5.70s\n",
      "        98      133910.7390            5.65s\n",
      "        99      133463.7037            5.59s\n",
      "       100      133031.0871            5.54s\n",
      "       101      132307.4042            5.48s\n",
      "       102      130632.9855            5.43s\n",
      "       103      129705.7455            5.37s\n",
      "       104      129357.8775            5.32s\n",
      "       105      129226.2206            5.28s\n",
      "       106      128824.6707            5.23s\n",
      "       107      128544.7221            5.17s\n",
      "       108      127542.9799            5.11s\n",
      "       109      126276.8464            5.06s\n",
      "       110      126088.3609            5.00s\n",
      "       111      125414.6191            4.95s\n",
      "       112      125068.4097            4.89s\n",
      "       113      124593.0991            4.84s\n",
      "       114      124333.7797            4.78s\n",
      "       115      124059.4122            4.73s\n",
      "       116      123541.4180            4.68s\n",
      "       117      123462.6728            4.63s\n",
      "       118      123016.9866            4.58s\n",
      "       119      122828.1040            4.52s\n",
      "       120      122655.0300            4.46s\n",
      "       121      122402.2313            4.41s\n",
      "       122      122168.7457            4.35s\n",
      "       123      121639.3573            4.29s\n",
      "       124      121368.7500            4.24s\n",
      "       125      120413.5439            4.19s\n",
      "       126      119566.3373            4.13s\n",
      "       127      118632.7383            4.07s\n",
      "       128      118177.7325            4.02s\n",
      "       129      117767.9747            3.97s\n",
      "       130      117123.6727            3.91s\n",
      "       131      116597.6414            3.85s\n",
      "       132      116392.7391            3.80s\n",
      "       133      115788.8326            3.75s\n",
      "       134      114744.6782            3.69s\n",
      "       135      114127.4605            3.63s\n",
      "       136      113915.5197            3.58s\n",
      "       137      113391.5669            3.52s\n",
      "       138      113110.6737            3.47s\n",
      "       139      111981.2078            3.41s\n",
      "       140      111690.3363            3.36s\n",
      "       141      111239.1168            3.31s\n",
      "       142      110980.2278            3.25s\n",
      "       143      110648.2046            3.19s\n",
      "       144      110071.4563            3.14s\n",
      "       145      109702.9634            3.09s\n",
      "       146      109526.3319            3.03s\n",
      "       147      109139.0848            2.97s\n",
      "       148      108958.9411            2.92s\n",
      "       149      108775.8375            2.86s\n",
      "       150      108657.7956            2.80s\n",
      "       151      108358.7873            2.75s\n",
      "       152      108181.8968            2.69s\n",
      "       153      107445.4943            2.64s\n",
      "       154      106945.9107            2.58s\n",
      "       155      106477.1296            2.53s\n",
      "       156      106307.9655            2.47s\n",
      "       157      106172.7082            2.42s\n",
      "       158      105772.1707            2.36s\n",
      "       159      105067.2731            2.30s\n",
      "       160      104650.1829            2.25s\n",
      "       161      104411.2367            2.19s\n",
      "       162      104295.9651            2.14s\n",
      "       163      103660.4769            2.08s\n",
      "       164      103315.6249            2.02s\n",
      "       165      102536.6161            1.97s\n",
      "       166      102470.1352            1.91s\n",
      "       167      102048.4233            1.85s\n",
      "       168      101899.4550            1.80s\n",
      "       169      101804.1638            1.74s\n",
      "       170      101630.9247            1.69s\n",
      "       171      101229.2195            1.63s\n",
      "       172      100744.6290            1.57s\n",
      "       173      100186.1844            1.52s\n",
      "       174      100082.1744            1.46s\n",
      "       175       99751.1728            1.41s\n",
      "       176       99405.5179            1.35s\n",
      "       177       99177.8210            1.29s\n",
      "       178       99119.9508            1.24s\n",
      "       179       98976.4886            1.18s\n",
      "       180       98685.4787            1.13s\n",
      "       181       98410.5863            1.07s\n",
      "       182       98342.5049            1.01s\n",
      "       183       97930.9970            0.96s\n",
      "       184       97420.3656            0.90s\n",
      "       185       97131.7448            0.85s\n",
      "       186       96996.7046            0.79s\n",
      "       187       96811.0158            0.73s\n",
      "       188       96785.5517            0.68s\n",
      "       189       96734.1409            0.62s\n",
      "       190       96536.9936            0.56s\n",
      "       191       96315.0203            0.51s\n",
      "       192       95512.2876            0.45s\n",
      "       193       95139.3596            0.39s\n",
      "       194       94830.2613            0.34s\n",
      "       195       94771.3939            0.28s\n",
      "       196       94466.7347            0.23s\n",
      "       197       94387.0299            0.17s\n",
      "       198       94240.2653            0.11s\n",
      "       199       93889.7677            0.06s\n",
      "       200       93661.4811            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1    12866686.8347           12.84s\n",
      "         2    10517072.7519           12.08s\n",
      "         3     8608437.3274           11.70s\n",
      "         4     7060690.9093           11.95s\n",
      "         5     5802007.2108           11.49s\n",
      "         6     4774106.0818           11.13s\n",
      "         7     3941652.2235           11.01s\n",
      "         8     3262911.6230           11.17s\n",
      "         9     2711180.0222           10.99s\n",
      "        10     2261851.6553           10.80s\n",
      "        11     1894142.1213           10.63s\n",
      "        12     1593143.8072           10.59s\n",
      "        13     1347132.2750           10.45s\n",
      "        14     1146306.8967           10.31s\n",
      "        15      982258.1304           10.20s\n",
      "        16      846944.8078           10.29s\n",
      "        17      736246.1723           10.22s\n",
      "        18      646279.7303           10.10s\n",
      "        19      572183.9455            9.99s\n",
      "        20      510913.8783            9.96s\n",
      "        21      460356.3387            9.86s\n",
      "        22      418656.4105            9.76s\n",
      "        23      382707.5722            9.66s\n",
      "        24      353717.1482            9.59s\n",
      "        25      328830.6191            9.52s\n",
      "        26      308195.0849            9.43s\n",
      "        27      291074.2738            9.36s\n",
      "        28      277128.3754            9.30s\n",
      "        29      265259.8486            9.24s\n",
      "        30      254754.1174            9.16s\n",
      "        31      245391.0276            9.09s\n",
      "        32      237337.6946            9.10s\n",
      "        33      230726.3314            9.05s\n",
      "        34      224383.6311            8.98s\n",
      "        35      219078.5804            8.94s\n",
      "        36      215425.7381            8.93s\n",
      "        37      211057.5793            8.85s\n",
      "        38      207622.4882            8.78s\n",
      "        39      204441.0253            8.72s\n",
      "        40      201735.9483            8.67s\n",
      "        41      199352.9151            8.61s\n",
      "        42      195984.9615            8.54s\n",
      "        43      194264.6353            8.48s\n",
      "        44      192314.8535            8.48s\n",
      "        45      190101.5750            8.43s\n",
      "        46      188155.6178            8.36s\n",
      "        47      186236.8230            8.30s\n",
      "        48      185097.6567            8.26s\n",
      "        49      183472.0451            8.19s\n",
      "        50      182037.3684            8.13s\n",
      "        51      180238.0728            8.08s\n",
      "        52      178695.8667            8.05s\n",
      "        53      177400.0875            8.00s\n",
      "        54      176056.6534            7.94s\n",
      "        55      174423.7109            7.90s\n",
      "        56      172794.7737            7.89s\n",
      "        57      171937.0597            7.83s\n",
      "        58      170192.3474            7.76s\n",
      "        59      169390.6935            7.71s\n",
      "        60      167532.3386            7.66s\n",
      "        61      166286.7075            7.61s\n",
      "        62      164911.5325            7.55s\n",
      "        63      163695.7154            7.50s\n",
      "        64      162145.5998            7.49s\n",
      "        65      161430.7670            7.43s\n",
      "        66      160244.1532            7.37s\n",
      "        67      159022.2857            7.31s\n",
      "        68      157975.8739            7.26s\n",
      "        69      156127.7609            7.20s\n",
      "        70      155052.6721            7.15s\n",
      "        71      154452.8059            7.10s\n",
      "        72      153577.1689            7.07s\n",
      "        73      151602.4558            7.02s\n",
      "        74      151097.2479            6.95s\n",
      "        75      150515.9342            6.90s\n",
      "        76      149508.1545            6.86s\n",
      "        77      148583.2244            6.80s\n",
      "        78      147741.9197            6.74s\n",
      "        79      147063.9874            6.68s\n",
      "        80      146138.8200            6.65s\n",
      "        81      145318.8504            6.59s\n",
      "        82      144149.7861            6.53s\n",
      "        83      142612.2511            6.47s\n",
      "        84      142028.9266            6.43s\n",
      "        85      141404.6774            6.36s\n",
      "        86      141102.5712            6.30s\n",
      "        87      140125.6958            6.25s\n",
      "        88      138643.4379            6.20s\n",
      "        89      137984.2008            6.14s\n",
      "        90      137480.0780            6.08s\n",
      "        91      136172.1830            6.03s\n",
      "        92      135452.9326            6.00s\n",
      "        93      135080.7737            5.94s\n",
      "        94      134798.4031            5.88s\n",
      "        95      134210.5714            5.82s\n",
      "        96      133723.9391            5.77s\n",
      "        97      132818.9593            5.71s\n",
      "        98      132004.7530            5.66s\n",
      "        99      131460.3625            5.61s\n",
      "       100      130518.0235            5.56s\n",
      "       101      129991.0723            5.50s\n",
      "       102      128688.3333            5.45s\n",
      "       103      128489.0435            5.39s\n",
      "       104      127836.6885            5.33s\n",
      "       105      127008.2469            5.28s\n",
      "       106      126708.9391            5.22s\n",
      "       107      126315.6001            5.17s\n",
      "       108      125934.8021            5.13s\n",
      "       109      124741.5802            5.08s\n",
      "       110      123931.9012            5.04s\n",
      "       111      123620.4924            4.99s\n",
      "       112      122992.3588            4.94s\n",
      "       113      122310.3017            4.90s\n",
      "       114      122058.7788            4.85s\n",
      "       115      121866.1173            4.80s\n",
      "       116      121607.0539            4.74s\n",
      "       117      120935.1341            4.68s\n",
      "       118      120724.9484            4.63s\n",
      "       119      120366.8955            4.58s\n",
      "       120      119841.9301            4.52s\n",
      "       121      119345.6754            4.46s\n",
      "       122      118675.6739            4.41s\n",
      "       123      118576.8648            4.37s\n",
      "       124      118411.9090            4.33s\n",
      "       125      117960.4136            4.27s\n",
      "       126      117104.0942            4.22s\n",
      "       127      116156.8073            4.16s\n",
      "       128      115580.5765            4.10s\n",
      "       129      114826.4245            4.04s\n",
      "       130      114301.0307            3.99s\n",
      "       131      114140.0107            3.93s\n",
      "       132      114005.5922            3.87s\n",
      "       133      113184.6395            3.81s\n",
      "       134      112863.6817            3.76s\n",
      "       135      112244.9688            3.70s\n",
      "       136      111937.1010            3.65s\n",
      "       137      111507.0767            3.59s\n",
      "       138      111313.9272            3.53s\n",
      "       139      110657.4885            3.47s\n",
      "       140      109713.4208            3.42s\n",
      "       141      109393.3338            3.36s\n",
      "       142      109042.1968            3.30s\n",
      "       143      108851.8582            3.24s\n",
      "       144      108146.6999            3.19s\n",
      "       145      108072.7902            3.13s\n",
      "       146      107241.9741            3.08s\n",
      "       147      106756.5220            3.02s\n",
      "       148      106634.1122            2.96s\n",
      "       149      106518.1773            2.91s\n",
      "       150      106456.9900            2.86s\n",
      "       151      106357.8306            2.80s\n",
      "       152      106172.7920            2.74s\n",
      "       153      105960.6041            2.69s\n",
      "       154      105757.4569            2.63s\n",
      "       155      105403.1163            2.57s\n",
      "       156      105184.9174            2.51s\n",
      "       157      104819.3582            2.46s\n",
      "       158      104392.5832            2.40s\n",
      "       159      104234.6383            2.35s\n",
      "       160      103776.6519            2.29s\n",
      "       161      103603.7034            2.23s\n",
      "       162      103177.5723            2.18s\n",
      "       163      102847.7333            2.12s\n",
      "       164      102242.1783            2.06s\n",
      "       165      101641.0626            2.00s\n",
      "       166      101554.9669            1.95s\n",
      "       167      101352.6080            1.89s\n",
      "       168      100723.9060            1.83s\n",
      "       169      100659.3264            1.77s\n",
      "       170      100153.0816            1.72s\n",
      "       171       99999.9590            1.66s\n",
      "       172       99821.4692            1.60s\n",
      "       173       99183.4199            1.54s\n",
      "       174       98637.9700            1.49s\n",
      "       175       98201.2387            1.43s\n",
      "       176       97814.4096            1.37s\n",
      "       177       97642.2497            1.31s\n",
      "       178       97098.4551            1.26s\n",
      "       179       96577.8895            1.20s\n",
      "       180       96095.7190            1.14s\n",
      "       181       95904.1813            1.09s\n",
      "       182       95638.3194            1.03s\n",
      "       183       95110.8008            0.97s\n",
      "       184       94480.3548            0.91s\n",
      "       185       94344.3530            0.86s\n",
      "       186       94162.4779            0.80s\n",
      "       187       93813.2787            0.74s\n",
      "       188       93497.8132            0.69s\n",
      "       189       92945.5064            0.63s\n",
      "       190       92734.7649            0.57s\n",
      "       191       92363.0728            0.51s\n",
      "       192       92312.6809            0.46s\n",
      "       193       92015.4454            0.40s\n",
      "       194       91551.1909            0.34s\n",
      "       195       91423.7030            0.29s\n",
      "       196       91277.9784            0.23s\n",
      "       197       90871.8597            0.17s\n",
      "       198       90589.5804            0.11s\n",
      "       199       90494.1347            0.06s\n",
      "       200       90136.4764            0.00s\n",
      "Cross validation score : 0.983\n",
      "[0.98157573 0.98227952 0.98059382 0.98294578 0.98134103]\n",
      "mae: 255.937345\n",
      "R2: 0.983161\n",
      "mse: 265103.228428\n",
      "rmse: 514.881762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   58.2s finished\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(estimator=gbr_model, X=X_train, y=y_train, cv=5,verbose = 1)\n",
    "print('Cross validation score : %.3f' % gbr_model.score(X_test, y_test))\n",
    "print(cv_score)\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "print(\"mae: %f\" %(mae))\n",
    "r2 = gbr_model.score(X_test,y_test)\n",
    "print(\"R2: %f\" %(r2))\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"mse: %f\" %(mse))\n",
    "rmse = (mean_squared_error(y_test, y_pred))**0.5\n",
    "print(\"rmse: %f\" %(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=100, depth=8\n",
    "Cross validation score : 0.981\n",
    "[0.98324256 0.9826404  0.9822319  0.97931829 0.98165835]\n",
    "mae: 264.090517\n",
    "R2: 0.982064\n",
    "mse: 286951.932481\n",
    "rmse: 535.678945\n",
    "\n",
    "Cross validation score : 0.982\n",
    "[0.98290342 0.98162418 0.98218219 0.97854537 0.98101767]\n",
    "mae: 266.467269\n",
    "R2: 0.982228\n",
    "mse: 284328.172392\n",
    "rmse: 533.224317\n",
    "max_depth = 200 \n",
    "rmse: 531.976353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_model = HistGradientBoostingRegressor(max_depth=10, max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr_model.fit(X_train, y_train)\n",
    "y_pred = hgbr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score : 0.9837\n",
      "[0.98263537 0.98269    0.98213042 0.98366576 0.9816379 ]\n",
      "mae: 258.030845\n",
      "R2: 1.0000\n",
      "mse: 256072.135040\n",
      "rmse: 506.035705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_val_score(estimator=hgbr_model, X=X_train, y=y_train, cv=5,verbose = 1)\n",
    "print('Cross validation score : %.4f' % hgbr_model.score(X_test, y_test))\n",
    "print(cv_score)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"mae: %f\" %(mae))\n",
    "r2 = hgbr_model.score(X_test,y_pred)\n",
    "print(\"R2: %.4f\" %(r2))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"mse: %f\" %(mse))\n",
    "rmse = (mean_squared_error(y_test, y_pred))**0.5\n",
    "print(\"rmse: %f\" %(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COn l/w y sinn nada \n",
    "Cross validation score : 0.9822\n",
    "[0.98363749 0.98244627 0.98260417 0.98031694 0.98236592]\n",
    "mae: 277.914486\n",
    "R2: 1.0000\n",
    "mse: 285311.164230\n",
    "rmse: 534.145265\n",
    "#max iter =200\n",
    "Cross validation score : 0.9824\n",
    "[0.98266068 0.98168035 0.98244195 0.97939003 0.98154611]\n",
    "mae: 277.697224\n",
    "R2: 1.0000\n",
    "mse: 281983.359959\n",
    "rmse: 531.021054\n",
    "#max_iter =300\n",
    "Cross validation score : 0.9823\n",
    "[0.98247221 0.98136899 0.98257238 0.97945764 0.98136524]\n",
    "mae: 274.923396\n",
    "R2: 1.0000\n",
    "mse: 282725.056504\n",
    "rmse: 531.718964\n",
    "\n",
    "#max iter = 200\n",
    "Cross validation score : 0.9822\n",
    "[0.98283749 0.98146759 0.98250352 0.97946861 0.98103671]\n",
    "mae: 279.372035\n",
    "R2: 1.0000\n",
    "mse: 285130.738400\n",
    "rmse: 533.976346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA TREES REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr_model = ExtraTreesRegressor()\n",
    "etr_model.fit(X_train, y_train)\n",
    "y_pred = etr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(estimator=etr_model, X=X_train, y=y_train, cv=5,verbose = 1)\n",
    "print('Cross validation score : %.4f' % etr_model.score(X_test, y_test))\n",
    "print(cv_score)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"mae: %f\" %(mae))\n",
    "r2 = etr_model.score(X_test,y_pred)\n",
    "print(\"R2: %.4f\" %(r2))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"mse: %f\" %(mse))\n",
    "rmse = (mean_squared_error(y_test, y_pred))**0.5\n",
    "print(\"rmse: %f\" %(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-39ceaba79b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdiamonds_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiamonds_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ironhack_env/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \"\"\"\n\u001b[0;32m-> 1650\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# In regression we can directly return the raw value from the trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ironhack_env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ironhack_env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ironhack_env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "X_predict = X_pred\n",
    "predictions = gbr_model.predict(X_predict)\n",
    "diamonds_id = diamonds_predict['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13485 entries, 0 to 13484\n",
      "Data columns (total 20 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           13485 non-null  int64  \n",
      " 1   carat        13485 non-null  float64\n",
      " 2   cut          13485 non-null  object \n",
      " 3   color        13485 non-null  object \n",
      " 4   clarity      13485 non-null  object \n",
      " 5   depth        13485 non-null  float64\n",
      " 6   table        13485 non-null  float64\n",
      " 7   x            13485 non-null  float64\n",
      " 8   y            13485 non-null  float64\n",
      " 9   z            13485 non-null  float64\n",
      " 10  volume       13485 non-null  float64\n",
      " 11  l/w ratio    13482 non-null  float64\n",
      " 12  depth ratio  13482 non-null  float64\n",
      " 13  carat log    13485 non-null  float64\n",
      " 14  cut_num      13485 non-null  int64  \n",
      " 15  color_num    13485 non-null  int64  \n",
      " 16  clarity_num  13485 non-null  int64  \n",
      " 17  cut/crt      13485 non-null  float64\n",
      " 18  color/crt    13485 non-null  float64\n",
      " 19  clarity/crt  13485 non-null  float64\n",
      "dtypes: float64(13), int64(4), object(3)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAR DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': diamonds_id, 'price': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13485, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id'] = submission['id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13485 entries, 0 to 13484\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      13485 non-null  int64  \n",
      " 1   price   13485 non-null  float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 210.8 KB\n"
     ]
    }
   ],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack_env]",
   "language": "python",
   "name": "conda-env-ironhack_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
